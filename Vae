Setup
For this lab, we will be using the following libraries:

pandas for managing the data.
numpy for mathematical operations.
sklearn for machine learning and machine-learning-pipeline related functions.
seaborn for visualizing the data.
matplotlib for additional plotting tools.
Installing Required Libraries
The following required libraries are pre-installed in the Skills Network Labs environment. However, if you run these notebook commands in a different Jupyter environment (like Watson Studio or Ananconda), you will need to install these libraries by removing the # sign before !mamba in the code cell below.

# All Libraries required for this lab are listed below. The libraries pre-installed on Skills Network Labs are commented.
# !mamba install -qy pandas==1.3.4
# All Libraries required for this lab are listed below. The libraries pre-installed on Skills Network Labs are commented.
# !mamba install -qy pandas==1.3.4 numpy==1.21.4 seaborn==0.9.0 matplotlib==3.5.0 scikit-learn==0.20.1
# Note: If your environment doesn't support "!mamba install", use "!pip install"
Run the following upgrade and then RESTART YOUR KERNEL. Make sure the version of tensorflow imported below is no less than 2.9.0.

%%capture
!pip3 install --upgrade tensorflow
Importing Required Libraries
We recommend you import all required libraries in one place (here):

# You can use this section to suppress warnings generated by your code:
def warn(*args, **kwargs):
    pass
import warnings
warnings.warn = warn
warnings.filterwarnings('ignore')
â€‹
import os
import numpy as np
â€‹
# Import the keras library
import tensorflow as tf
print(tf.__version__)
from tensorflow import keras
from tensorflow.keras import Model
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense,Layer,Reshape,Conv2DTranspose
from tensorflow.python.client import device_lib
from keras.layers import Multiply, Add
from keras import backend as K
â€‹
from numpy import random
â€‹
from matplotlib import pyplot as plt
Defining Helper Functions
def plot_label_clusters(model, data, labels):
    # display a 2D plot of the digit classes in the latent space
    z_mean, _, _ =encoder.predict(data)
    plt.figure(figsize=(8, 6))
    plt.scatter(z_mean[:, 0], z_mean[:, 1], c=labels)
    plt.colorbar()
    plt.xlabel("z[0]")
    plt.ylabel("z[1]")
    plt.show()
Preparing the Dataset
We load the MNIST handwritten digit dataset:

# Import data
(X_train, y_train), (_, _) = keras.datasets.mnist.load_data()
There are 60000  28Ã—28
  images in the training set:

X_train.shape, y_train.shape
Let's look at the unique labels of the digits that we want to predict:

np.unique(y_train)
We will reshape the training set to  60000Ã—28Ã—28Ã—1
  to work with convolutions. 1 indicates that the input images only have one channel, that is: grayscale images.

print(f"Before reshaping, X_train has a shape of: {X_train.shape}")
â€‹
X_train = X_train.reshape((X_train.shape[0],X_train.shape[1],X_train.shape[2],1))
print(f"After reshaping, X_train has a shape of: {X_train.shape}")
We cast the data type of X_train to tf.float32 and normalize its values to range from 0 to 1:

X_train = tf.cast(X_train, tf.float32)
X_train = X_train/255.0
We convert the tensors to a tf.data.Dataset object.

dataset=tf.data.Dataset.from_tensor_slices(X_train)
dataset
We can plot five random samples from the training set:

for r in random.randint(0, 59999, size=5, dtype=int):
    
    plt.figure(figsize=(3,3))
    plt.imshow(X_train[r,:,:,0],cmap="gray")
    plt.title("sample No.{} belongs to class {}".format(r,y_train[r]))
    plt.axis("off")
Variational Autoencoder
In a nutshell, the architecture of a VAE is similar to that of a standard Autoencoder such that it consists of an encoder and a decoder, both of which are trained to minimize the reconstruction error between the original data  ğ‘‹
  and the encoded-decoded data  ğ‘‹Ì‚ 
 .

The training samples  ğ‘‹
  are passed to the encoder to generate samples  ğ’›
 , which are mappings of  ğ‘‹
  in the latent space. The decoder then uses  ğ’›
  to generate the most likely reconstruction  ğ‘‹Ì‚ 
 .

At this point, a natural question that comes in mind is, how do we use VAEs to generate meaningful content? You might think that if we train a VAE on images, then we can use it to generate new images. However, since it's difficult to regularize what happens to our encoder output in the latent space, we can't be sure that the encoder will organize information in the latent space in a way that the decoder can easily take that information and generates content that seems reasonable.

Hence, in order to be able to use the VAE for meaningful generative purposes, we need to introduce some regularization into the latent space. As shown in the diagram below, instead of mapping the input to a single point in the latent space, we encode it as a normal distribution by returning the mean and standard deviation of the distribution. By doing so, the decoder would be able to use the regularized information to construct new content.

In the following sections, we will build a VAE by following the architecture in the illustrative diagram below.

computer components
Encoder Part
We build the encoder part by breaking it up into three smaller parts.

The first part of the encoder consists of several fully connected layers, as shown in the picture below, which encodes the high-dimensional input; For our implementation, we will use two convolution layers since we want the model to learn the image data using convolutions.

computer components
Here is the code for building the first part of the encoder. The encoder_output that comes out of the convolution layers is denoted as  ğ’›2
 .

encoder_input= keras.Input(shape=(28, 28, 1))
x = Conv2D(32, 3, activation="relu", strides=2, padding="same")(encoder_input)
x = Conv2D(64, 3, activation="relu", strides=2, padding="same")(x)
x = Flatten()(x)
encoder_output = Dense(16, activation="relu")(x)
The second part of the encoder represents a normal distribution over the latent space that takes the encoder_output and gives you the probability of it belonging to the distribution.

The mean and standard deviation of the normal distribution will be learned and then used to calculate the log-likelihood for optimization purposes.

computer components
To implement the second part, we create two Dense layers in parallel for the model to learn the mean and log variance respectively; We want the model to learn the log variance instead of the variance because it brings more stability and ease of training. The detailed reason is as follows:

By definition,  ğœ
  is a non-negative real number. To enforce this, we would need to use the ReLU activation to obtain such a value, but the gradient is not well defined around zero.
Besides, since we typically apply normalization methods in model training, the data values range from 0 to 1, which means the standard deviation of those values is also very small. This adds a burden to the optimization process and causes numerical instabilities as the gradients flowing in the backpropagation will contain floating points.
Note that we can convert the log variance to the standard deviation using the exponential function. Here is the code for building the second part:

latent_dim = 2
â€‹
# Dense layer to learn the mean
mean = Dense(latent_dim, name="mean")(encoder_output)
# Dense layer to learn the log variance
log_var = Dense(latent_dim, name="z_log_var")(encoder_output)
â€‹
# sigmia is calculated from log variance
sigma = tf.exp(0.5 * log_var)
Here comes the third part of the encoder, where we sample a point from the learned distribution in the latent space. The sampled point will later be decoded by the decoder to generate new content.

The following diagram illustrates the random sampling:

computer components
The sampled point, denoted by  ğ’›5
  in the diagram, comes from a normal distribution with mean  ğœ‡
  and standard deviation  ğœ
 , with some random noise  ğœ–
 .

# random normal noise 
epsilon = K.random_normal(shape = (tf.shape(mean)[0], tf.shape(mean)[1]))
Due to the random sampling of  ğ’›5
 , we use the Reparameterization Trick  ğ‘§=ğœ‡+ğœâŠ™ğœ–
 , which allows the VAE to backpropagate through a random node.

#z = mean + sigma * epsilon 
 
z_eps = Multiply()([sigma, epsilon])
z = Add()([mean, z_eps])
The encoder will output the mean and log_var of the learned distribution as well as the sampled point z. Now, we create the complete encoder network by chaining the encoder_input and the outputs:

encoder = Model(encoder_input, outputs = [mean, log_var, z], name = 'encoder')
encoder.summary()
Decoder Part
The decoder part of a VAE is the same as that of a regular Autoencoder. To upsample the output from the encoder, we can use the Keras Sequential model API to group a stack of Dense layers and Transpose Convolution layers together.

computer components
latent_dim=2
decoder=Sequential()
â€‹
decoder.add(keras.Input(shape=(latent_dim,))) # input dimension is 2
decoder.add(Dense(7 * 7 * 64, activation="relu"))
decoder.add(Reshape((7, 7, 64)))
decoder.add(Conv2DTranspose(64, 3, activation="relu", strides=2, padding="same"))
decoder.add(Conv2DTranspose(32, 3, activation="relu", strides=2, padding="same"))
decoder.add(Conv2DTranspose(1, 3, activation="sigmoid", padding="same"))
decoder.summary()
Loss Function
We include the reconstruction loss, which compares the output of the VAE to the original input. In this case, we use the mean square error. We also include a regularization term, which regularizes the organization of the latent space by making the distribution learned by the encoder close to a standard normal distribution. The regularization is enforced using the Kullbackâ€“Leibler divergence that compares two distributions.

# make loss function 
â€‹
#reconstruction_loss
def reconstruction_loss(y, y_hat):
    return tf.reduce_mean(tf.square(y - y_hat))
â€‹
â€‹
#Kullbackâ€“Leibler divergence encoder loss
def kl_loss(mu, log_var):
    loss = -0.5 * tf.reduce_mean(1 + log_var - tf.square(mean) - tf.exp(log_var))
    return loss
â€‹
# add two losses 
def vae_loss(y_true, y_hat, mu, log_var):
    return reconstruction_loss(y_true, y_hat) + (1 / (64*64)) * kl_loss(mean, log_var)
â€‹
Putting it all Together
We combine the Encoder and Decoder to create a VAE, and append the regularization term to the model:

# encoder returns mean and log variance of the normal distribution,
# and a sample point z
mean, log_var, z = encoder(encoder_input)
â€‹
# decoder decodes the sample z 
reconstructed = decoder(z)
â€‹
model = Model(encoder_input, reconstructed, name ="vae")
loss = kl_loss(mean, log_var)
model.add_loss(loss)
model.summary()
Training the VAE
We now train the model. After each epoch, we input random noise z into the decoder. We will see for each epoch, the decoder output will look more and more like a digit.

We also output the values for the latent space z for all the different digits in our dataset, color coded according to class. We see that for each iteration, the samples appeared more clustered.

#loss
mse_losses = []
kl_losses = []
# optimizer 
optimizer =  tf.keras.optimizers.Adam(learning_rate = 0.0002, beta_1 = 0.5, beta_2 = 0.999 )
epochs = 5
â€‹
for epoch in range(epochs):
    
    print(f"Samples generated by decoder after {epoch} epoch(s): ")
    
    #random noise
    z = tf.random.normal(shape = (5, latent_dim,))
â€‹
    # input random noise into the decoder
    xhat = decoder.predict(z)
    
    # plot the decoder output
    plt.figure()
    for i in range(5):
        plt.subplot(1,5,i+1)
        plt.imshow(xhat[i,:,:,0],cmap="gray")
        plt.axis("off")
    plt.show()
â€‹
    print(f"2D latent representations of the training data produced by encoder after {epoch} epoch(s): ")
    plot_label_clusters(encoder, X_train, y_train)
â€‹
â€‹
    # training steps
    for (step, training_batch) in enumerate(dataset.batch(100)):
        with tf.GradientTape() as tape:
â€‹
            # model output
            reconstructed = model(training_batch)
â€‹
            y_true = tf.reshape(training_batch, shape = [-1])
            y_pred = tf.reshape(reconstructed, shape = [-1])
â€‹
            # calculate reconstruction loss
            mse_loss = reconstruction_loss(y_true, y_pred)
            # calculate KL divergence
            kl = sum(model.losses)
â€‹
            kl_losses.append(kl.numpy())
            mse_losses.append(mse_loss .numpy())
â€‹
            # total loss
            train_loss = 0.01 * kl + mse_loss
â€‹
            grads = tape.gradient(train_loss, model.trainable_variables)
            optimizer.apply_gradients(zip(grads, model.trainable_variables))
         
    print("Epoch: %s - Step: %s - MSE loss: %s - KL loss: %s" % (epoch, step, mse_loss.numpy(), kl.numpy()))
  
We plot the reconstruction loss and Kullbackâ€“Leibler divergence against the number of training iterations, we can see that they are both decreasing.

plt.plot(mse_losses)
plt.title(" reconstruction loss ")
plt.show()
plt.plot(kl_losses)
plt.title("  Kullbackâ€“Leibler divergence")
plt.show()
Now that our VAE has been trained, we can use its decoder network to generate some samples. Let's see if our decoder can do a good job in generating artificial images that look like digits!

xhat = decoder.predict(z)
â€‹
for i in range(5):
    plt.imshow(xhat[i,:,:,0],cmap="gray")
    plt.show()
We can also visualize the final latent representations of the training data produced by the trained encoder on a 2D plot, colored by classes.

plot_label_clusters(encoder, X_train, y_train)
